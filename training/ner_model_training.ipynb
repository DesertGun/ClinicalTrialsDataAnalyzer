{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import spacy, random\r\n",
    "import spacy_annotator as spa\r\n",
    "import numpy as np\r\n",
    "from spacy.util import minibatch, compounding\r\n",
    "from pathlib import Path\r\n",
    "from spacy.training import Example\r\n",
    "from spacy import displacy\r\n",
    "import pandas as pd\r\n",
    "from training_data.custom_training_data import train_data_init"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Init GPU-Usage, load blank model with basis of en_core_web_sm and\r\n",
    "# enable only NER in the pipeline\r\n",
    "\r\n",
    "spacy.prefer_gpu()\r\n",
    "nlp = spacy.blank('en')\r\n",
    "source_nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "nlp.add_pipe(\"ner\", source=source_nlp)\r\n",
    "\r\n",
    "# Init tracker for training loss with traing_data\r\n",
    "\r\n",
    "current_loss = 0\r\n",
    "training_data_init = train_data_init"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "'''\r\n",
    "@author Ervin Joa\r\n",
    "\r\n",
    "Init-Training of a custom NER-Model with droprate of 20%\r\n",
    "\r\n",
    "@return current_loss\r\n",
    "'''\r\n",
    "def train_ner_model(trainig_data_init): \r\n",
    "\r\n",
    "    examples = []\r\n",
    "    losses = {}\r\n",
    "    for text, annots in trainig_data_init:\r\n",
    "        examples.append(Example.from_dict(nlp.make_doc(text), annots))\r\n",
    "\r\n",
    "        \r\n",
    "    optimizer = nlp.initialize(lambda: examples)\r\n",
    "\r\n",
    "    for i in range(40):\r\n",
    "        random.shuffle(examples)\r\n",
    "        for batch in minibatch(examples, size = compounding(1., 32., 1.001)):\r\n",
    "            nlp.update(\r\n",
    "                batch,\r\n",
    "                drop=0.2,\r\n",
    "                sgd=optimizer,\r\n",
    "                losses = losses,\r\n",
    "            )\r\n",
    "    print(\"Losses: \", losses)\r\n",
    "    current_loss = losses[\"ner\"]\r\n",
    "\r\n",
    "    nlp.to_disk(\"../training/ner_model\")\r\n",
    "    return current_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "current_loss = train_ner_model(training_data_init)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Losses:  {'ner': 4598.098713291725}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "'''\r\n",
    "@author Ervin Joa\r\n",
    "\r\n",
    "Method for the continuation training with SpaCy. After\r\n",
    "each tme this function is called, the model will be retrained and\r\n",
    "updated for the data-inputed.\r\n",
    "\r\n",
    "@param train_data: Training Data based on the GOLD-Format\r\n",
    "@return current_loss: Loss of the current training epoch\r\n",
    "'''\r\n",
    "\r\n",
    "def resume_train_ner_model(train_data):\r\n",
    "    nlp = spacy.load(\"../training/ner_model\")\r\n",
    "\r\n",
    "    examples = []\r\n",
    "    losses = {}\r\n",
    "    for text, annots in train_data:\r\n",
    "        examples.append(Example.from_dict(nlp.make_doc(text), annots))\r\n",
    "\r\n",
    "    optimizer_resume = nlp.resume_training()\r\n",
    "\r\n",
    "    for i in range(int(np.sqrt(len(train_data)))):\r\n",
    "        random.shuffle(examples)\r\n",
    "        for batch in minibatch(examples, size = compounding(1.0, 32.0, 1.001)):\r\n",
    "            nlp.update(\r\n",
    "                batch,\r\n",
    "                drop=0.2,\r\n",
    "                sgd=optimizer_resume,\r\n",
    "                losses = losses,\r\n",
    "            )\r\n",
    "    current_loss = losses[\"ner\"]\r\n",
    "\r\n",
    "    nlp.to_disk(\"../training/ner_model\")\r\n",
    "    return current_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def from_csv_to_train_data(datadf, label, colname):\r\n",
    "    output = []\r\n",
    "    datadf[colname] = datadf[colname].astype(str)\r\n",
    "    for index, row in datadf.iterrows():\r\n",
    "        char_start = 0\r\n",
    "        char_end = len(row[colname])\r\n",
    "        output_row = (row[colname], {\"entities\":[(char_start, char_end, label)]})\r\n",
    "        output.append(output_row)\r\n",
    "    return output\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def filter_dataframe_count(df, col_name):\r\n",
    "    df['word_count'] = df[col_name].str.split().str.len()\r\n",
    "    df_new = df[(df['word_count'] <= 4) & (df['word_count'] > 1)].drop(columns=[\"word_count\"], axis=0)\r\n",
    "\r\n",
    "    return df_new.reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "data_vars = pd.read_csv(\"../training/training_data/varaible_data.csv\")\r\n",
    "data_refs = pd.read_csv(\"../training/training_data/reference_data.csv\")\r\n",
    "data_timepoint_1 = pd.read_csv(\"../training/training_data/timepoint_data.csv\")\r\n",
    "data_change = pd.read_csv(\"../training/training_data/change_data.csv\")\r\n",
    "data_condition = pd.read_csv(\"../training/training_data/condition_data.csv\")\r\n",
    "\r\n",
    "data_vars.dropna(inplace=True)\r\n",
    "data_refs.dropna(inplace=True)\r\n",
    "data_timepoint_1.dropna(inplace=True)\r\n",
    "data_change.dropna(inplace=True)\r\n",
    "data_condition.dropna(inplace=True)\r\n",
    "\r\n",
    "# For all Files filter data-entries that are <= 4 words\r\n",
    "\r\n",
    "data_vars = filter_dataframe_count(data_vars, data_vars.iloc[:, 0].name)\r\n",
    "data_timepoint_1 = filter_dataframe_count(data_timepoint_1, data_timepoint_1.iloc[:, 0].name)\r\n",
    "data_change = filter_dataframe_count(data_change, data_change.iloc[:, 0].name)\r\n",
    "data_condition = filter_dataframe_count(data_condition, data_condition.iloc[:, 0].name)\r\n",
    "\r\n",
    "data_var_training = from_csv_to_train_data(data_vars, \"Variable\", \"units_analyzed\")\r\n",
    "data_ref_training = from_csv_to_train_data(data_refs, \"Reference\", \"param_type\")\r\n",
    "data_timepoint_training = from_csv_to_train_data(data_timepoint_1, \"Timepoint\", \"target_duration\")\r\n",
    "data_change_training = from_csv_to_train_data(data_change, \"Change\", \"units\")\r\n",
    "data_condition_training = from_csv_to_train_data(data_condition, \"Condition\", \"name\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(len(data_var_training))\r\n",
    "print(len(data_ref_training))\r\n",
    "print(len(data_timepoint_training))\r\n",
    "print(len(data_change_training))\r\n",
    "print(len(data_condition_training))\r\n",
    "print(len(train_data_init))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "663\n",
      "13\n",
      "162\n",
      "17744\n",
      "64226\n",
      "128\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "'''\r\n",
    "@author Ervin Joa\r\n",
    "\r\n",
    "Main training-loop:\r\n",
    "\r\n",
    "    While the loss of training remains above 100:\r\n",
    "        continue extracting random shuffled training data and retrain the model;\r\n",
    "    else:\r\n",
    "        stop;\r\n",
    "'''\r\n",
    "\r\n",
    "while current_loss > 100:\r\n",
    "    # Due to huge amount of data, randomize and select first 200 entries\r\n",
    "    data_var_training_random = random.sample(data_var_training, len(data_var_training))\r\n",
    "    data_change_training_random = random.sample(data_change_training, len(data_change_training))\r\n",
    "    data_condition_training_random = random.sample(data_condition_training, len(data_condition_training))\r\n",
    "\r\n",
    "    # Creation of the training dataset for training \r\n",
    "    training_data = data_ref_training + data_change_training_random[0:200] + data_timepoint_training + \\\r\n",
    "    data_var_training_random[0:200] + data_condition_training_random[0:200] + train_data_init\r\n",
    "\r\n",
    "    # Shuffeling and selecting the first 500 entries for training\r\n",
    "    data_to_train = random.sample(training_data, len(training_data))\r\n",
    "    data_to_train_portion = data_to_train[0:500]\r\n",
    "\r\n",
    "    # Training and display of the traing-loss for each session\r\n",
    "    current_loss = resume_train_ner_model(data_to_train_portion)\r\n",
    "    print(f\"Done Training iteration with loss of {current_loss}\")\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done Training iteration with loss of 3780.66520294482\n",
      "Done Training iteration with loss of 2996.661206510561\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-cfb79b7c4b1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# Training and display of the traing-loss for each session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mcurrent_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresume_train_ner_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_train_portion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Done Training iteration with loss of {current_loss}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-1a200c6b8eca>\u001b[0m in \u001b[0;36mresume_train_ner_model\u001b[1;34m(train_data)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompounding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             nlp.update(\n\u001b[0m\u001b[0;32m     26\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                     \u001b[1;32mand\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m                 ):\n\u001b[1;32m-> 1131\u001b[1;33m                     \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinish_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mannotates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m                 for doc, eg in zip(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.finish_update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36mfinish_update\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m                     param, grad = optimizer(\n\u001b[0m\u001b[0;32m    328\u001b[0m                         \u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                     )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\thinc\\optimizers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, key, weights, gradient, lr_scale)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr_scale\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[0mgradient\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL2\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL2_is_weight_decay\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr_scale\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#nlp_annot = spacy.load(\"../training/ner_model\")\r\n",
    "#data_filtered_raw= pd.read_csv(\"../backend/input_data_filtered.csv\")\r\n",
    "#annotator = spa.Annotator(labels = [\"Condition\", \"Reference\", \"Change\", \"Timepoint\", \"Variable\"], model = nlp_annot)\r\n",
    "#df_labels = annotator.annotate(df = data_filtered_raw[0:50], col_text = \"PrimaryOutcomeMeasure\", shuffle = True)\r\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "092e3a7a52c84567be0bf7fd7cde921f"
      },
      "text/plain": [
       "HTML(value='-1 examples annotated, 51 examples left')"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97201effca144808a8e9bd186bed5021"
      },
      "text/plain": [
       "Text(value='', description='Condition', layout=Layout(width='auto'), placeholder='ent one, ent two, ent three'…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "455f91af90f14e99804a5b02a66a1b70"
      },
      "text/plain": [
       "Text(value='', description='Reference', layout=Layout(width='auto'), placeholder='ent one, ent two, ent three'…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "356c70dd5bfe411b85cdd3b3a6c66c85"
      },
      "text/plain": [
       "Text(value='', description='Change', layout=Layout(width='auto'), placeholder='ent one, ent two, ent three')"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f2313aef635426682d881aa218c1364"
      },
      "text/plain": [
       "Text(value='', description='Timepoint', layout=Layout(width='auto'), placeholder='ent one, ent two, ent three'…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5aed558b8f564f22a85d3f7c7a977e52"
      },
      "text/plain": [
       "Text(value='', description='Variable', layout=Layout(width='auto'), placeholder='ent one, ent two, ent three')"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71bc79198e1341b8ac6e5f13a1391572"
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='submit', style=ButtonStyle()), Button(button_style=…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35c59f20243842268343a74654d7e0d0"
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(list(df_labels[\"annotations\"]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '', ('Number of patients who died for fournier gangrene', {'entities': [(32, 49, 'Condition'), (0, 18, 'Change')]}), ('Cheeks appearance', {'entities': [(0, 17, 'Reference')]}), ('Number of patients without intubation', {'entities': [(0, 18, 'Change'), (27, 37, 'Variable')]}), '', '', ('Percentage of Participants With Serious Adverse Events (SAEs) and Serious Adverse Drug Reactions (SADRs)|Percentage of Participants With Unexpected Adverse Events (AEs) and Adverse Drug Reactions (ADRs) Not Mentioned in Precautions|Percentage of Participants With Expected/Already Known ADRs at Week 13|Percentage of Participants With Expected/Already Known ADRs at Week 26|Percentage of Participants With Expected/Already Known ADRs at Week 39|Percentage of Participants With Expected/Already Known ADRs at Week 52|Percentage of Participants With Expected/Already Known ADRs at Week 153|Percentage of Participants With Non-serious ADRs|Percentage of Participants With Abnormal Laboratory Findings Reported as AEs', {'entities': [(0, 26, 'Change'), (32, 61, 'Variable'), (137, 168, 'Variable'), (173, 202, 'Variable'), (264, 291, 'Variable'), (335, 362, 'Variable'), (406, 433, 'Variable'), (477, 504, 'Variable'), (548, 575, 'Variable'), (669, 697, 'Variable')]}), '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nlp_test = spacy.load(\"../training/ner_model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for text, _ in data_to_train[0:50]:\r\n",
    "\r\n",
    "        target = nlp_test(text)\r\n",
    "        for entity in target.ents:\r\n",
    "                displacy.render(nlp_test(target.text), style='ent')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "from spacy.scorer import Scorer\r\n",
    "\r\n",
    "def evaluate( examples):\r\n",
    "    ner_model = spacy.load(\"../training/ner_model\")\r\n",
    "    scorer = Scorer()\r\n",
    "\r\n",
    "\r\n",
    "    examples = []\r\n",
    "    for text, annots in examples:\r\n",
    "        predicted=ner_model(text)\r\n",
    "        example=Example.from_dict(predicted, annots)\r\n",
    "        examples.append(example)\r\n",
    "    scores = scorer.score(examples)\r\n",
    "    return scores\r\n",
    "\r\n",
    "examples = [\r\n",
    "    ('The main outcome is the comparison of total volumetric bone mineral density (vBMD) at the tibia and distal radius',[(9,16, \"Change\"), (24, 34, \"Reference\"),(38,82, \"Variable\")]),\r\n",
    "    (\"Number of Participants with undiagnosed type 2 diabetes\", [(0, 22,\"Change\"), (40, 55, \"Condition\")]),\r\n",
    "    (\"2 months\", [(0,8,\"Timepoint\")])\r\n",
    "]\r\n",
    "\r\n",
    "\r\n",
    "results = evaluate(examples)\r\n",
    "print(results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'token_acc': None, 'token_p': None, 'token_r': None, 'token_f': None, 'sents_p': None, 'sents_r': None, 'sents_f': None, 'tag_acc': None, 'pos_acc': None, 'morph_acc': None, 'morph_per_feat': None, 'dep_uas': None, 'dep_las': None, 'dep_las_per_type': None, 'ents_p': None, 'ents_r': None, 'ents_f': None, 'ents_per_type': None, 'cats_score': 0.0, 'cats_score_desc': 'macro F', 'cats_micro_p': 0.0, 'cats_micro_r': 0.0, 'cats_micro_f': 0.0, 'cats_macro_p': 0.0, 'cats_macro_r': 0.0, 'cats_macro_f': 0.0, 'cats_macro_auc': 0.0, 'cats_f_per_type': {}, 'cats_auc_per_type': {}}\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}