{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import spacy, random\r\n",
    "import spacy_annotator as spa\r\n",
    "import numpy as np\r\n",
    "from spacy.util import minibatch, compounding\r\n",
    "from pathlib import Path\r\n",
    "from spacy.training import Example\r\n",
    "from spacy import displacy\r\n",
    "import pandas as pd\r\n",
    "from training_data.custom_training_data import train_data_init"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Init GPU-Usage, load blank model with basis of en_core_web_sm and\r\n",
    "# enable only NER in the pipeline\r\n",
    "\r\n",
    "spacy.prefer_gpu()\r\n",
    "nlp = spacy.blank('en')\r\n",
    "source_nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "nlp.add_pipe(\"ner\", source=source_nlp)\r\n",
    "\r\n",
    "# Init tracker for training loss with traing_data\r\n",
    "\r\n",
    "current_loss = 0\r\n",
    "training_data_init = train_data_init"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "'''\r\n",
    "@author Ervin Joa\r\n",
    "\r\n",
    "Init-Training of a custom NER-Model with droprate of 20%\r\n",
    "\r\n",
    "@return current_loss\r\n",
    "'''\r\n",
    "def train_ner_model(trainig_data_init): \r\n",
    "\r\n",
    "    examples = []\r\n",
    "    losses = {}\r\n",
    "    for text, annots in trainig_data_init:\r\n",
    "        examples.append(Example.from_dict(nlp.make_doc(text), annots))\r\n",
    "\r\n",
    "        \r\n",
    "    optimizer = nlp.initialize(lambda: examples)\r\n",
    "\r\n",
    "    for i in range(35):\r\n",
    "        random.shuffle(examples)\r\n",
    "        for batch in minibatch(examples, size = compounding(1., 32., 1.001)):\r\n",
    "            nlp.update(\r\n",
    "                batch,\r\n",
    "                drop=0.2,\r\n",
    "                sgd=optimizer,\r\n",
    "                losses = losses,\r\n",
    "            )\r\n",
    "    print(\"Losses: \", losses)\r\n",
    "    current_loss = losses[\"ner\"]\r\n",
    "\r\n",
    "    nlp.to_disk(\"../training/ner_model\")\r\n",
    "    return current_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "current_loss = train_ner_model(training_data_init)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Losses:  {'ner': 3966.5003709342427}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "'''\r\n",
    "@author Ervin Joa\r\n",
    "\r\n",
    "Method for the continuation training with SpaCy. After\r\n",
    "each tme this function is called, the model will be retrained and\r\n",
    "updated for the data-inputed.\r\n",
    "\r\n",
    "@param train_data: Training Data based on the GOLD-Format\r\n",
    "@return current_loss: Loss of the current training epoch\r\n",
    "'''\r\n",
    "\r\n",
    "def resume_train_ner_model(train_data):\r\n",
    "    nlp = spacy.load(\"../training/ner_model\")\r\n",
    "\r\n",
    "    examples = []\r\n",
    "    losses = {}\r\n",
    "    for text, annots in train_data:\r\n",
    "        examples.append(Example.from_dict(nlp.make_doc(text), annots))\r\n",
    "\r\n",
    "    optimizer_resume = nlp.resume_training()\r\n",
    "\r\n",
    "    for i in range(40):\r\n",
    "        random.shuffle(examples)\r\n",
    "        for batch in minibatch(examples, size = compounding(1.0, 32.0, 1.001)):\r\n",
    "            nlp.update(\r\n",
    "                batch,\r\n",
    "                drop=0.2,\r\n",
    "                sgd=optimizer_resume,\r\n",
    "                losses = losses,\r\n",
    "            )\r\n",
    "    current_loss = losses[\"ner\"]\r\n",
    "\r\n",
    "    nlp.to_disk(\"../training/ner_model\")\r\n",
    "    return current_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def from_csv_to_train_data(datadf, label, colname):\r\n",
    "    output = []\r\n",
    "    datadf[colname] = datadf[colname].astype(str)\r\n",
    "    for index, row in datadf.iterrows():\r\n",
    "        char_start = 0\r\n",
    "        char_end = len(row[colname])\r\n",
    "        output_row = (row[colname], {\"entities\":[(char_start, char_end, label)]})\r\n",
    "        output.append(output_row)\r\n",
    "    return output\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def filter_dataframe_count(df, col_name):\r\n",
    "    df['word_count'] = df[col_name].str.split().str.len()\r\n",
    "    df_new = df[(df['word_count'] <= 4) & (df['word_count'] > 1)].drop(columns=[\"word_count\"], axis=0)\r\n",
    "\r\n",
    "    return df_new.reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "data_vars = pd.read_csv(\"../training/training_data/varaible_data.csv\")\r\n",
    "data_refs = pd.read_csv(\"../training/training_data/reference_data.csv\")\r\n",
    "data_timepoint_1 = pd.read_csv(\"../training/training_data/timepoint_data.csv\")\r\n",
    "data_change = pd.read_csv(\"../training/training_data/change_data.csv\")\r\n",
    "data_condition = pd.read_csv(\"../training/training_data/condition_data.csv\")\r\n",
    "\r\n",
    "data_vars.dropna(inplace=True)\r\n",
    "data_refs.dropna(inplace=True)\r\n",
    "data_timepoint_1.dropna(inplace=True)\r\n",
    "data_change.dropna(inplace=True)\r\n",
    "data_condition.dropna(inplace=True)\r\n",
    "\r\n",
    "# For all Files filter data-entries that are <= 4 words\r\n",
    "\r\n",
    "data_vars = filter_dataframe_count(data_vars, data_vars.iloc[:, 0].name)\r\n",
    "data_timepoint_1 = filter_dataframe_count(data_timepoint_1, data_timepoint_1.iloc[:, 0].name)\r\n",
    "data_change = filter_dataframe_count(data_change, data_change.iloc[:, 0].name)\r\n",
    "data_condition = filter_dataframe_count(data_condition, data_condition.iloc[:, 0].name)\r\n",
    "\r\n",
    "data_var_training = from_csv_to_train_data(data_vars, \"Variable\", \"units_analyzed\")\r\n",
    "data_ref_training = from_csv_to_train_data(data_refs, \"Reference\", \"param_type\")\r\n",
    "data_timepoint_training = from_csv_to_train_data(data_timepoint_1, \"Timepoint\", \"target_duration\")\r\n",
    "data_change_training = from_csv_to_train_data(data_change, \"Change\", \"units\")\r\n",
    "data_condition_training = from_csv_to_train_data(data_condition, \"Condition\", \"name\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(len(data_var_training))\r\n",
    "print(len(data_ref_training))\r\n",
    "print(len(data_timepoint_training))\r\n",
    "print(len(data_change_training))\r\n",
    "print(len(data_condition_training))\r\n",
    "print(len(train_data_init))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "663\n",
      "13\n",
      "162\n",
      "17744\n",
      "64226\n",
      "128\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "@author Ervin Joa\r\n",
    "\r\n",
    "Main training-loop:\r\n",
    "\r\n",
    "    While the loss of training remains above 100:\r\n",
    "        continue extracting random shuffled training data and retrain the model;\r\n",
    "    else:\r\n",
    "        stop;\r\n",
    "'''\r\n",
    "\r\n",
    "while current_loss > 100:\r\n",
    "    # Due to huge amount of data, randomize and select first 200 entries\r\n",
    "    data_var_training_random = random.sample(data_var_training, len(data_var_training))\r\n",
    "    data_change_training_random = random.sample(data_change_training, len(data_change_training))\r\n",
    "    data_condition_training_random = random.sample(data_condition_training, len(data_condition_training))\r\n",
    "\r\n",
    "    # Creation of the training dataset for training \r\n",
    "    training_data = data_ref_training + data_change_training_random[0:200] + data_timepoint_training + \\\r\n",
    "    data_var_training_random[0:200] + data_condition_training_random[0:200] + train_data_init\r\n",
    "\r\n",
    "    # Shuffeling and selecting the first 200 entries for training\r\n",
    "    data_to_train = random.sample(training_data, len(training_data))\r\n",
    "    data_to_train_portion = data_to_train[0:200]\r\n",
    "\r\n",
    "    # Training and display of the traing-loss for each session\r\n",
    "    current_loss = resume_train_ner_model(data_to_train_portion)\r\n",
    "    print(f\"Done Training iteration with loss of {current_loss}\")\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#nlp_annot = spacy.load(\"../training/ner_model\")\r\n",
    "#data_filtered_raw= pd.read_csv(\"../backend/input_data_filtered.csv\")\r\n",
    "#annotator = spa.Annotator(labels = [\"Condition\", \"Reference\", \"Change\", \"Timepoint\", \"Variable\"], model = nlp_annot)\r\n",
    "#df_labels = annotator.annotate(df = data_filtered_raw[0:50], col_text = \"PrimaryOutcomeMeasure\", shuffle = True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#print(list(df_labels[\"annotations\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nlp_test = spacy.load(\"../training/ner_model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for text, _ in data_to_train[0:50]:\r\n",
    "\r\n",
    "        target = nlp_test(text)\r\n",
    "        for entity in target.ents:\r\n",
    "                displacy.render(nlp_test(target.text), style='ent')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "from spacy.scorer import Scorer\r\n",
    "\r\n",
    "def evaluate( examples):\r\n",
    "    ner_model = spacy.load(\"../training/ner_model\")\r\n",
    "    scorer = Scorer()\r\n",
    "\r\n",
    "\r\n",
    "    examples = []\r\n",
    "    for text, annots in examples:\r\n",
    "        predicted=ner_model(text)\r\n",
    "        example=Example.from_dict(predicted, annots)\r\n",
    "        examples.append(example)\r\n",
    "    scores = scorer.score(examples)\r\n",
    "    return scores\r\n",
    "\r\n",
    "examples = [\r\n",
    "    ('The main outcome is the comparison of total volumetric bone mineral density (vBMD) at the tibia and distal radius',[(9,16, \"Change\"), (24, 34, \"Reference\"),(38,82, \"Variable\")]),\r\n",
    "    (\"Number of Participants with undiagnosed type 2 diabetes\", [(0, 22,\"Change\"), (40, 55, \"Condition\")]),\r\n",
    "    (\"2 months\", [(0,8,\"Timepoint\")])\r\n",
    "]\r\n",
    "\r\n",
    "\r\n",
    "results = evaluate(examples)\r\n",
    "print(results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'token_acc': None, 'token_p': None, 'token_r': None, 'token_f': None, 'sents_p': None, 'sents_r': None, 'sents_f': None, 'tag_acc': None, 'pos_acc': None, 'morph_acc': None, 'morph_per_feat': None, 'dep_uas': None, 'dep_las': None, 'dep_las_per_type': None, 'ents_p': None, 'ents_r': None, 'ents_f': None, 'ents_per_type': None, 'cats_score': 0.0, 'cats_score_desc': 'macro F', 'cats_micro_p': 0.0, 'cats_micro_r': 0.0, 'cats_micro_f': 0.0, 'cats_macro_p': 0.0, 'cats_macro_r': 0.0, 'cats_macro_f': 0.0, 'cats_macro_auc': 0.0, 'cats_f_per_type': {}, 'cats_auc_per_type': {}}\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa103bc93a6207848fa67a1515d71b9e40ee492733cad84c02b8df8f3c6dbc5a"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}