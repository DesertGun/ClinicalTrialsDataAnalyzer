{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import spacy_annotator as spa\n",
    "import numpy as np\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy.training import Example\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "from training_data.custom_training_data import train_data_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init GPU-Usage, load blank model with basis of en_core_web_sm and\n",
    "# enable only NER in the pipeline\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.blank('en')\n",
    "source_nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"ner\", source=source_nlp)\n",
    "\n",
    "# Init tracker for training loss with traing_data\n",
    "\n",
    "current_loss = 0\n",
    "training_data_init = train_data_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@author Ervin Joa\n",
    "\n",
    "Init-Training of a custom NER-Model with droprate of 20%\n",
    "\n",
    "@param: list of manually labeled training data\n",
    "\n",
    "@return current_loss\n",
    "'''\n",
    "\n",
    "\n",
    "def train_ner_model(trainig_data_init):\n",
    "\n",
    "    examples = []\n",
    "    losses = {}\n",
    "    for text, annots in trainig_data_init:\n",
    "        examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "\n",
    "    optimizer = nlp.initialize(lambda: examples)\n",
    "\n",
    "    for iter in range(50):\n",
    "        random.shuffle(examples)\n",
    "        for batch in minibatch(examples, size=compounding(1., 32., 1.001)):\n",
    "            nlp.update(\n",
    "                batch,\n",
    "                drop=0.2,\n",
    "                sgd=optimizer,\n",
    "                losses=losses,\n",
    "            )\n",
    "    print(\"Losses: \", losses)\n",
    "    current_loss = losses[\"ner\"]\n",
    "\n",
    "    nlp.to_disk(\"../training/ner_model\")\n",
    "    return current_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses:  {'ner': 8928.782519897488}\n"
     ]
    }
   ],
   "source": [
    "current_loss = train_ner_model(training_data_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@author Ervin Joa\n",
    "\n",
    "Method for the continuation training with SpaCy. After\n",
    "each tme this function is called, the model will be retrained and\n",
    "updated for the data-inputed.\n",
    "\n",
    "@param train_data: Training Data based on the GOLD-Format\n",
    "@return current_loss: Loss of the current training epoch\n",
    "'''\n",
    "\n",
    "\n",
    "def resume_train_ner_model(train_data):\n",
    "    nlp = spacy.load(\"../training/ner_model\")\n",
    "\n",
    "    examples = []\n",
    "    losses = {}\n",
    "    for text, annots in train_data:\n",
    "        examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "\n",
    "    optimizer_resume = nlp.resume_training()\n",
    "\n",
    "    for iter in range(50):\n",
    "        random.shuffle(examples)\n",
    "        for batch in minibatch(examples, size=compounding(1., 32., 1.001)):\n",
    "            nlp.update(\n",
    "                batch,\n",
    "                drop=0.2,\n",
    "                sgd=optimizer_resume,\n",
    "                losses=losses,\n",
    "            )\n",
    "    current_loss = losses[\"ner\"]\n",
    "\n",
    "    nlp.to_disk(\"../training/ner_model\")\n",
    "    return current_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@author Ervin Joa\n",
    "\n",
    "This method transformes a feature to the curresponding\n",
    "GOD-standard set of items, which will be used\n",
    "for SpaCys training.\n",
    "\n",
    "@param datadf: unput dataframe with the feature\n",
    "@param label: label to be given for entry\n",
    "@param colname: column name of the feature in the input dataframe\n",
    "\n",
    "@return list\n",
    "'''\n",
    "\n",
    "\n",
    "def from_csv_to_train_data(datadf, label, colname):\n",
    "    output = []\n",
    "    datadf[colname] = datadf[colname].astype(str)\n",
    "    for index, row in datadf.iterrows():\n",
    "        char_start = 0\n",
    "        char_end = len(row[colname])\n",
    "        output_row = (row[colname], {\"entities\": [\n",
    "                      (char_start, char_end, label)]})\n",
    "        output.append(output_row)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@author Ervin Joa\n",
    "\n",
    "Method fpr creating a new feature, with which the number\n",
    "of words per entry will be filtered.\n",
    "The goal is to avoid training large sentances, which can\n",
    "overfit the model and to only train keywords or entities whith\n",
    "multiple words beloging to it. e.g. \"Type 2 Diabetes\"\n",
    "\n",
    "@param df: dataframe containing the feature\n",
    "@param colname: column name of the feature in the input dataframe\n",
    "\n",
    "@return filtered dataframe\n",
    "'''\n",
    "\n",
    "\n",
    "def filter_dataframe_count(df, col_name):\n",
    "    df['word_count'] = df[col_name].str.split().str.len()\n",
    "    df_new = df[(df['word_count'] <= 4) & (df['word_count'] > 1)\n",
    "                ].drop(columns=[\"word_count\"], axis=0)\n",
    "\n",
    "    return df_new.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of the extracted data from ACCT\n",
    "data_vars = pd.read_csv(\"../training/training_data/varaible_data.csv\")\n",
    "data_refs = pd.read_csv(\"../training/training_data/reference_data.csv\")\n",
    "data_timepoint_1 = pd.read_csv(\"../training/training_data/timepoint_data.csv\")\n",
    "data_change = pd.read_csv(\"../training/training_data/change_data.csv\")\n",
    "data_condition = pd.read_csv(\"../training/training_data/condition_data.csv\")\n",
    "\n",
    "# droping empty entries\n",
    "data_vars.dropna(inplace=True)\n",
    "data_refs.dropna(inplace=True)\n",
    "data_timepoint_1.dropna(inplace=True)\n",
    "data_change.dropna(inplace=True)\n",
    "data_condition.dropna(inplace=True)\n",
    "\n",
    "# For all Files filter data-entries that are <= 4 words\n",
    "data_vars = filter_dataframe_count(data_vars, data_vars.iloc[:, 0].name)\n",
    "data_timepoint_1 = filter_dataframe_count(\n",
    "    data_timepoint_1, data_timepoint_1.iloc[:, 0].name)\n",
    "data_change = filter_dataframe_count(data_change, data_change.iloc[:, 0].name)\n",
    "data_condition = filter_dataframe_count(\n",
    "    data_condition, data_condition.iloc[:, 0].name)\n",
    "\n",
    "# Transfomration of the data to the corresponding training-format (GOLD-standard) lists\n",
    "data_var_training = from_csv_to_train_data(\n",
    "    data_vars, \"Variable\", \"units_analyzed\")\n",
    "data_ref_training = from_csv_to_train_data(\n",
    "    data_refs, \"Reference\", \"param_type\")\n",
    "data_timepoint_training = from_csv_to_train_data(\n",
    "    data_timepoint_1, \"Timepoint\", \"target_duration\")\n",
    "data_change_training = from_csv_to_train_data(data_change, \"Change\", \"units\")\n",
    "data_condition_training = from_csv_to_train_data(\n",
    "    data_condition, \"Condition\", \"name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Training iteration epoch Nr. 1 with loss of 2053.201431748315\n",
      "Done Training iteration epoch Nr. 2 with loss of 1416.739391536687\n",
      "Done Training iteration epoch Nr. 3 with loss of 1567.2378504293465\n",
      "Done Training iteration epoch Nr. 4 with loss of 1450.479927350355\n",
      "Done Training iteration epoch Nr. 5 with loss of 1045.8071162253398\n",
      "Done Training iteration epoch Nr. 6 with loss of 2061.7157966570703\n",
      "Done Training iteration epoch Nr. 7 with loss of 942.0956782751825\n",
      "Done Training iteration epoch Nr. 8 with loss of 1024.360110468681\n",
      "Done Training iteration epoch Nr. 9 with loss of 929.5616843582495\n",
      "Done Training iteration epoch Nr. 10 with loss of 783.3708781466682\n",
      "Done Training iteration epoch Nr. 11 with loss of 493.1286495073215\n",
      "Done Training iteration epoch Nr. 12 with loss of 819.1708668245018\n",
      "Done Training iteration epoch Nr. 13 with loss of 1254.6247862063992\n",
      "Done Training iteration epoch Nr. 14 with loss of 1031.9637345997496\n",
      "Done Training iteration epoch Nr. 15 with loss of 808.4945205881385\n",
      "Done Training iteration epoch Nr. 16 with loss of 990.6284462979227\n",
      "Done Training iteration epoch Nr. 17 with loss of 1202.3142811761409\n",
      "Done Training iteration epoch Nr. 18 with loss of 721.7018447036241\n",
      "Done Training iteration epoch Nr. 19 with loss of 1257.7924796911718\n",
      "Done Training iteration epoch Nr. 20 with loss of 802.6033510697879\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author Ervin Joa\n",
    "\n",
    "Main training-loop:\n",
    "\n",
    "    While the number of training-iterations < 21:\n",
    "        continue extracting random shuffled training data and retrain the model;\n",
    "'''\n",
    "num_training_iter = 1\n",
    "\n",
    "# 20 epoch, since ner-losses typically plateu after this value\n",
    "while num_training_iter < 21:\n",
    "    # Due to huge amount of data, randomize and select first 200 entries\n",
    "    data_var_training_random = random.sample(\n",
    "        data_var_training, len(data_var_training))\n",
    "    data_change_training_random = random.sample(\n",
    "        data_change_training, len(data_change_training))\n",
    "    data_condition_training_random = random.sample(\n",
    "        data_condition_training, len(data_condition_training))\n",
    "\n",
    "    # Creation of the training dataset for training\n",
    "    training_data = data_ref_training + data_change_training_random[0:200] + data_timepoint_training + \\\n",
    "        data_var_training_random[0:200] + \\\n",
    "        data_condition_training_random[0:200] + train_data_init\n",
    "\n",
    "    # Shuffeling and selecting the first 200 entries for training\n",
    "    data_to_train = random.sample(training_data, len(training_data))\n",
    "    data_to_train_portion = data_to_train[0:200]\n",
    "\n",
    "    # Training and display of the traing-loss for each session\n",
    "    current_loss = resume_train_ner_model(data_to_train_portion)\n",
    "    print(\n",
    "        f\"Done Training iteration epoch Nr. {num_training_iter} with loss of {current_loss}\")\n",
    "    num_training_iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the usage of the SpaCy annotator\n",
    "\n",
    "#nlp_annot = spacy.load(\"../training/ner_model\")\n",
    "#data_filtered_raw= pd.read_csv(\"../backend/input_data_filtered.csv\")\n",
    "#annotator = spa.Annotator(labels = [\"Condition\", \"Reference\", \"Change\", \"Timepoint\", \"Variable\"], model = nlp_annot)\n",
    "#df_labels = annotator.annotate(df = data_filtered_raw[0:100], col_text = \"PrimaryOutcomeMeasure\", shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(df_labels[\"annotations\"]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
