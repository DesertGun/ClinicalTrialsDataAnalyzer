{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import spacy, random\r\n",
    "import spacy_annotator as spa\r\n",
    "import numpy as np\r\n",
    "from spacy.util import minibatch, compounding\r\n",
    "from pathlib import Path\r\n",
    "from spacy.training import Example\r\n",
    "from spacy import displacy\r\n",
    "import pandas as pd\r\n",
    "from training_data.custom_training_data import train_data_init"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Init GPU-Usage, load blank model with basis of en_core_web_sm and\r\n",
    "# enable only NER in the pipeline\r\n",
    "\r\n",
    "spacy.prefer_gpu()\r\n",
    "nlp = spacy.blank('en')\r\n",
    "source_nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "nlp.add_pipe(\"ner\", source=source_nlp)\r\n",
    "\r\n",
    "# Init tracker for training loss with traing_data\r\n",
    "\r\n",
    "current_loss = 0\r\n",
    "training_data_init = train_data_init"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<spacy.pipeline.ner.EntityRecognizer at 0x2220d511ca0>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "'''\r\n",
    "@author Ervin Joa\r\n",
    "\r\n",
    "Init-Training of a custom NER-Model with droprate of 20%\r\n",
    "\r\n",
    "@return current_loss\r\n",
    "'''\r\n",
    "def train_ner_model(trainig_data_init): \r\n",
    "\r\n",
    "    examples = []\r\n",
    "    losses = {}\r\n",
    "    for text, annots in trainig_data_init:\r\n",
    "        examples.append(Example.from_dict(nlp.make_doc(text), annots))\r\n",
    "\r\n",
    "        \r\n",
    "    optimizer = nlp.initialize(lambda: examples)\r\n",
    "\r\n",
    "    for i in range(30):\r\n",
    "        random.shuffle(examples)\r\n",
    "        for batch in minibatch(examples, size = compounding(1., 32., 1.001)):\r\n",
    "            nlp.update(\r\n",
    "                batch,\r\n",
    "                drop=0.2,\r\n",
    "                sgd=optimizer,\r\n",
    "                losses = losses,\r\n",
    "            )\r\n",
    "    print(\"Losses: \", losses)\r\n",
    "    current_loss = losses[\"ner\"]\r\n",
    "\r\n",
    "    nlp.to_disk(\"../training/ner_model\")\r\n",
    "    return current_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "current_loss = train_ner_model(training_data_init)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Losses:  {'ner': 3411.014173047471}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "'''\r\n",
    "@author Ervin Joa\r\n",
    "\r\n",
    "Method for the continuation training with SpaCy. After\r\n",
    "each tme this function is called, the model will be retrained and\r\n",
    "updated for the data-inputed.\r\n",
    "\r\n",
    "@param train_data: Training Data based on the GOLD-Format\r\n",
    "@return current_loss: Loss of the current training epoch\r\n",
    "'''\r\n",
    "\r\n",
    "def resume_train_ner_model(train_data):\r\n",
    "    nlp = spacy.load(\"../training/ner_model\")\r\n",
    "\r\n",
    "    examples = []\r\n",
    "    losses = {}\r\n",
    "    for text, annots in train_data:\r\n",
    "        examples.append(Example.from_dict(nlp.make_doc(text), annots))\r\n",
    "\r\n",
    "    optimizer_resume = nlp.resume_training()\r\n",
    "\r\n",
    "    for i in range(int(np.sqrt(len(train_data)))):\r\n",
    "        random.shuffle(examples)\r\n",
    "        for batch in minibatch(examples, size = compounding(1.0, 32.0, 1.001)):\r\n",
    "            nlp.update(\r\n",
    "                batch,\r\n",
    "                drop=0.2,\r\n",
    "                sgd=optimizer_resume,\r\n",
    "                losses = losses,\r\n",
    "            )\r\n",
    "    current_loss = losses[\"ner\"]\r\n",
    "\r\n",
    "    nlp.to_disk(\"../training/ner_model\")\r\n",
    "    return current_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def from_csv_to_train_data(datadf, label, colname):\r\n",
    "    output = []\r\n",
    "    datadf[colname] = datadf[colname].astype(str)\r\n",
    "    for index, row in datadf.iterrows():\r\n",
    "        char_start = 0\r\n",
    "        char_end = len(row[colname])\r\n",
    "        output_row = (row[colname], {\"entities\":[(char_start, char_end, label)]})\r\n",
    "        output.append(output_row)\r\n",
    "    return output\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def filter_dataframe_count(df, col_name):\r\n",
    "    df['word_count'] = df[col_name].str.split().str.len()\r\n",
    "    df_new = df[df['word_count'] <= 2].drop(columns=[\"word_count\"], axis=0)\r\n",
    "\r\n",
    "    return df_new.reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "data_vars = pd.read_csv(\"../training/training_data/varaible_data.csv\")\r\n",
    "data_refs = pd.read_csv(\"../training/training_data/reference_data.csv\")\r\n",
    "data_timepoint_1 = pd.read_csv(\"../training/training_data/timepoint_data.csv\")\r\n",
    "data_change = pd.read_csv(\"../training/training_data/change_data.csv\")\r\n",
    "data_condition = pd.read_csv(\"../training/training_data/condition_data.csv\")\r\n",
    "\r\n",
    "data_vars.dropna(inplace=True)\r\n",
    "data_refs.dropna(inplace=True)\r\n",
    "data_timepoint_1.dropna(inplace=True)\r\n",
    "data_change.dropna(inplace=True)\r\n",
    "data_condition.dropna(inplace=True)\r\n",
    "\r\n",
    "# For all Files filter data-entries that are <= 4 words\r\n",
    "\r\n",
    "data_vars = filter_dataframe_count(data_vars, data_vars.iloc[:, 0].name)\r\n",
    "data_refs = filter_dataframe_count(data_refs, data_refs.iloc[:, 0].name)\r\n",
    "data_timepoint_1 = filter_dataframe_count(data_timepoint_1, data_timepoint_1.iloc[:, 0].name)\r\n",
    "data_change = filter_dataframe_count(data_change, data_change.iloc[:, 0].name)\r\n",
    "data_condition = filter_dataframe_count(data_condition, data_condition.iloc[:, 0].name)\r\n",
    "\r\n",
    "data_var_training = from_csv_to_train_data(data_vars, \"Variable\", \"units_analyzed\")\r\n",
    "data_ref_training = from_csv_to_train_data(data_refs, \"Reference\", \"param_type\")\r\n",
    "data_timepoint_training = from_csv_to_train_data(data_timepoint_1, \"Timepoint\", \"target_duration\")\r\n",
    "data_change_training = from_csv_to_train_data(data_change, \"Change\", \"units\")\r\n",
    "data_condition_training = from_csv_to_train_data(data_condition, \"Condition\", \"name\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(len(data_var_training))\r\n",
    "print(len(data_ref_training))\r\n",
    "print(len(data_timepoint_training))\r\n",
    "print(len(data_change_training))\r\n",
    "print(len(data_condition_training))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "682\n",
      "9\n",
      "162\n",
      "10251\n",
      "35602\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "'''\r\n",
    "@author Ervin Joa\r\n",
    "\r\n",
    "Main training-loop:\r\n",
    "\r\n",
    "    While the loss of training remains above 100:\r\n",
    "        continue extracting random shuffled training data and retrain the model;\r\n",
    "    else:\r\n",
    "        stop;\r\n",
    "'''\r\n",
    "\r\n",
    "while current_loss > 100:\r\n",
    "    data_var_training_random = random.sample(data_var_training, len(data_var_training))\r\n",
    "    data_change_training_random = random.sample(data_change_training, len(data_change_training))\r\n",
    "    data_condition_training_random = random.sample(data_condition_training, len(data_condition_training))\r\n",
    "\r\n",
    "    training_data = data_ref_training + data_change_training_random[0:300] + data_timepoint_training + data_var_training_random[0:300] + data_condition_training_random[0:300] + train_data_init\r\n",
    "    data_to_train = random.sample(training_data, len(training_data))\r\n",
    "    \r\n",
    "    data_to_train_portion = data_to_train[0:500]\r\n",
    "\r\n",
    "    current_loss = resume_train_ner_model(data_to_train_portion)\r\n",
    "    print(f\"Done Training iteration with loss of {current_loss}\")\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('Microhaemorrhage', {'entities': [(0, 16, 'Condition')]}), ('fallopian tubes', {'entities': [(0, 15, 'Variable')]}), ('Change in baseline insulin satisfaction at 12 months', {'entities': [(0, 6, 'Change'), (10, 18, 'Reference'), (19, 39, 'Variable'), (43, 52, 'Timepoint')]}), ('Change in HbA1c', {'entities': [(0, 15, 'Change')]}), ('Kilogram', {'entities': [(0, 8, 'Change')]}), ('Nasal Intubation', {'entities': [(0, 16, 'Condition')]}), ('Unit/kilogram (U/kg)', {'entities': [(0, 20, 'Change')]}), ('Pg/mL', {'entities': [(0, 5, 'Change')]}), ('8 Years', {'entities': [(0, 7, 'Timepoint')]}), ('collisions', {'entities': [(0, 10, 'Change')]})]\n",
      "Done Training iteration with loss of 3379.1869744131377\n",
      "[('Target Lesions', {'entities': [(0, 14, 'Variable')]}), ('pen needles', {'entities': [(0, 11, 'Variable')]}), ('Primary Glioblastoma', {'entities': [(0, 20, 'Condition')]}), ('Dialyzers', {'entities': [(0, 9, 'Variable')]}), ('Afterschool Programs', {'entities': [(0, 20, 'Variable')]}), ('Barthel score', {'entities': [(0, 13, 'Change')]}), ('Tachograms', {'entities': [(0, 10, 'Variable')]}), ('Pulmonary Procedures', {'entities': [(0, 20, 'Condition')]}), ('Elderly PatiÃ«nts', {'entities': [(0, 16, 'Condition')]}), ('Grammes', {'entities': [(0, 7, 'Change')]})]\n",
      "Done Training iteration with loss of 2224.793273655447\n",
      "[('HIV, Inflammation', {'entities': [(0, 17, 'Condition')]}), ('Pharmacy', {'entities': [(0, 8, 'Variable')]}), ('Proteolytic System', {'entities': [(0, 18, 'Condition')]}), ('Hip Effusion', {'entities': [(0, 12, 'Condition')]}), ('log mg/ml', {'entities': [(0, 9, 'Change')]}), ('Hepatocellular Carcinoma.', {'entities': [(0, 25, 'Condition')]}), ('Chronic Bronchiectasis', {'entities': [(0, 22, 'Condition')]}), ('53 Months', {'entities': [(0, 9, 'Timepoint')]}), ('sessions', {'entities': [(0, 8, 'Variable')]}), ('mL/Kg/min', {'entities': [(0, 9, 'Change')]})]\n",
      "Done Training iteration with loss of 2010.3382972378129\n",
      "[('Adjusted Mean Change From Baseline in Fasting Plasma Glucose at Week 24 (Last Observation Carried Forward [LOCF])', {'entities': [(0, 20, 'Change'), (26, 34, 'Reference'), (38, 60, 'Variable'), (64, 71, 'Timepoint')]}), ('Physician Communication', {'entities': [(0, 23, 'Condition')]}), ('Images', {'entities': [(0, 6, 'Variable')]}), ('Number', {'entities': [(0, 6, 'Reference')]}), ('Nimotuzumab', {'entities': [(0, 11, 'Condition')]}), ('Questionnaires', {'entities': [(0, 14, 'Variable')]}), ('Treatments', {'entities': [(0, 10, 'Variable')]}), ('Treatment groups', {'entities': [(0, 16, 'Variable')]}), ('7 Years', {'entities': [(0, 7, 'Timepoint')]}), ('Depressive Episode', {'entities': [(0, 18, 'Condition')]})]\n",
      "Done Training iteration with loss of 1215.798828350549\n",
      "[('Tooth surfaces', {'entities': [(0, 14, 'Variable')]}), ('pulmonary veins', {'entities': [(0, 15, 'Variable')]}), ('ng equivalents*h/ml', {'entities': [(0, 19, 'Change')]}), ('Case Management', {'entities': [(0, 15, 'Condition')]}), ('Sport Injury', {'entities': [(0, 12, 'Condition')]}), ('HAART Non-Adherence', {'entities': [(0, 19, 'Condition')]}), ('Gallbladder Carcinoma', {'entities': [(0, 21, 'Condition')]}), ('CCTA Scans', {'entities': [(0, 10, 'Variable')]}), ('bleeds', {'entities': [(0, 6, 'Variable')]}), ('24 Years', {'entities': [(0, 8, 'Timepoint')]})]\n",
      "Done Training iteration with loss of 1279.8006921844662\n",
      "[('Hair follicles', {'entities': [(0, 14, 'Change')]}), ('Recurrence, Detection rate per patient of PET-PSMA for the detection of biochemical recurrence.', {'entities': [(0, 10, 'Change'), (12, 38, 'Reference'), (42, 50, 'Variable'), (72, 94, 'Variable')]}), ('12 Days', {'entities': [(0, 7, 'Timepoint')]}), ('Patient Records', {'entities': [(0, 15, 'Variable')]}), ('Units (U)/kg', {'entities': [(0, 12, 'Change')]}), ('Millimoles/liter', {'entities': [(0, 16, 'Change')]}), ('Juvenile Myopia', {'entities': [(0, 15, 'Condition')]}), ('Fold increase', {'entities': [(0, 13, 'Change')]}), ('Percent Saturation', {'entities': [(0, 18, 'Change')]}), ('Migraine attacks', {'entities': [(0, 16, 'Variable')]})]\n",
      "Done Training iteration with loss of 1369.78641661252\n",
      "[('MINUTES', {'entities': [(0, 7, 'Change')]}), ('ABI Value', {'entities': [(0, 9, 'Change')]}), ('Surveys', {'entities': [(0, 7, 'Variable')]}), ('Lymph nodes', {'entities': [(0, 11, 'Variable')]}), ('manual maps', {'entities': [(0, 11, 'Variable')]}), ('Visit Connections', {'entities': [(0, 17, 'Variable')]}), ('Polysomnographic recording of nocturnal sleep', {'entities': [(17, 45, 'Variable')]}), ('ears', {'entities': [(0, 4, 'Variable')]}), ('events', {'entities': [(0, 6, 'Variable')]}), ('Abstinence Syndrome', {'entities': [(0, 19, 'Condition')]})]\n",
      "Done Training iteration with loss of 950.1970640566644\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# nlp_annot = spacy.load(\"../training/ner_model\")\r\n",
    "# data_filtered_raw= pd.read_csv(\"../backend/input_data_filtered.csv\")\r\n",
    "# annotator = spa.Annotator(labels = [\"Condition\", \"Reference\", \"Change\", \"Timepoint\", \"Variable\"], model = nlp_annot)\r\n",
    "# df_labels = annotator.annotate(df = data_filtered_raw[0:25], col_text = \"SecondaryOutcomeMeasure\", shuffle = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "nlp_test = spacy.load(\"../training/ner_model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "doc = nlp_test(\"To evaluate a nutritional intervention for women newly diagnosed with breast cancer 9 weight control and physical activity program\")\r\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Entities [('To evaluate a nutritional intervention', 'Change'), ('cancer 9 weight control', 'Variable'), ('physical activity program', 'Change')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for text, _ in data_to_train_portion:\r\n",
    "\r\n",
    "        target = nlp_test(text)\r\n",
    "        for entity in target.ents:\r\n",
    "                displacy.render(nlp_test(target.text), style='ent')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\r\n",
    "from spacy.scorer import Scorer\r\n",
    "\r\n",
    "def evaluate( examples):\r\n",
    "    ner_model = spacy.load(\"../training/ner_model\")\r\n",
    "    scorer = Scorer()\r\n",
    "\r\n",
    "\r\n",
    "    examples = []\r\n",
    "    for text, annots in examples:\r\n",
    "        predicted=ner_model(text)\r\n",
    "        example=Example.from_dict(predicted, annots)\r\n",
    "        examples.append(example)\r\n",
    "    scores = scorer.score(examples)\r\n",
    "    return scores\r\n",
    "\r\n",
    "examples = [\r\n",
    "    ('The main outcome is the comparison of total volumetric bone mineral density (vBMD) at the tibia and distal radius',[(9,16, \"Change\"), (24, 34, \"Reference\"),(38,82, \"Variable\")]),\r\n",
    "    (\"Number of Participants with undiagnosed type 2 diabetes\", [(0, 22,\"Change\"), (40, 55, \"Condition\")]),\r\n",
    "    (\"2 months\", [(0,8,\"Timepoint\")])\r\n",
    "]\r\n",
    "\r\n",
    "\r\n",
    "results = evaluate(examples)\r\n",
    "print(results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'token_acc': None, 'token_p': None, 'token_r': None, 'token_f': None, 'sents_p': None, 'sents_r': None, 'sents_f': None, 'tag_acc': None, 'pos_acc': None, 'morph_acc': None, 'morph_per_feat': None, 'dep_uas': None, 'dep_las': None, 'dep_las_per_type': None, 'ents_p': None, 'ents_r': None, 'ents_f': None, 'ents_per_type': None, 'cats_score': 0.0, 'cats_score_desc': 'macro F', 'cats_micro_p': 0.0, 'cats_micro_r': 0.0, 'cats_micro_f': 0.0, 'cats_macro_p': 0.0, 'cats_macro_r': 0.0, 'cats_macro_f': 0.0, 'cats_macro_auc': 0.0, 'cats_f_per_type': {}, 'cats_auc_per_type': {}}\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}